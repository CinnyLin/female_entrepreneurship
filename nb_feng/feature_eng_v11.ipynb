{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature engineering\n",
    "\n",
    "*dealing with multi-label columns, which generates the most columns*\n",
    "\n",
    "v3\n",
    "- instead of encoding the `female_led` info, just keep the percentage; if no information, encode **0.5**\n",
    "- no multilabelbinarizers\n",
    "\n",
    "v4\n",
    "- only keep the `Industry Groups` multilabel column\n",
    "\n",
    "v5\n",
    "- break down `Headquarters Location` multilabel column and use OneHotEncoder instead (similar to how `Headquarters Region` was processed\n",
    "\n",
    "v6 \n",
    "- strategically add `Top 5 Investors` information in after pre-processing, take investors appearing more than **10** times (added 78 more columns)\n",
    "\n",
    "v7\n",
    "- `Headquarters Location` expanded to also include cities\n",
    "- take `Top 5 Investors` appearing more than **5** times\n",
    "\n",
    "*the xgboost and ols models are still performing poorly*\n",
    "\n",
    "v8\n",
    "- take `Top 5 Investors` appearing more than **3** times\n",
    "- include both `Industries` and `Industry Groups`\n",
    "\n",
    "v8.2\n",
    "- remove \"cities\" info from `Headquarters Location`\n",
    "\n",
    "v9\n",
    "- improved location encoding, i.e. keep different levels of location information so that the distribution is also relatively more balanced: {western us: 571, beijing: 393, europe_country: 324, united kingdom: 290, northeastern us: 258, chinese_state: 230, shanghai: 198, france: 183, guangdong: 179, southern us: 121, germany: 105, scandinavia: 98, midwestern us: 50}\n",
    "\n",
    "v10\n",
    "- add industry encoding, top level encoding!\n",
    "    - \"top 5 industry groups\" (bool): count > 500 --> sexy hot industries\n",
    "    - \"out of top 50 industries how many each company belongs to\" --> diversified across industries\n",
    "- also add top level investor encoding with same logic!\n",
    "- v10.1: only keep industry groups (turns out not as successful)\n",
    "- v10.2: only keep top industries\n",
    "\n",
    "v11 \n",
    "- dropna %female for classification\n",
    "- drop cb ranking and trend scores because they could be correlated with funding\n",
    "- only keep top industries because keeping all would be too many cols for the smaller df we have after dropna\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# feature engineering\n",
    "from numpy import asarray\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, MultiLabelBinarizer\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Aggregate Data\n",
    "combine data of regions China, Europe, and the US."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 113)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regions = ['China', 'Europe', 'US']\n",
    "\n",
    "df0 = pd.read_csv(f'../data/crunchbase-aggregated/{regions[0]}-gender.csv')\n",
    "df1 = pd.read_csv(f'../data/crunchbase-aggregated/{regions[1]}-gender.csv')\n",
    "df2 = pd.read_csv(f'../data/crunchbase-aggregated/{regions[2]}-gender.csv')\n",
    "\n",
    "df = pd.concat([df0, df1, df2])\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Transformation\n",
    "data scaling, discretization, dealing missing values etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 skip rows\n",
    "\n",
    "1. rows already correctly labeled\n",
    "2. all the same or too many NULLs\n",
    "3. equivalent to name\n",
    "4. equivalent to total funding amount\n",
    "5. irrelevant data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_cols = ['Number of Founders']\n",
    "df.rename(columns={'Number of Founders': 'number_of_founders'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 38)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop_cols = ['Description', 'Full Description', \n",
    "             'Website', 'Twitter', 'Facebook', 'LinkedIn',\n",
    "             'Contact Email', 'Phone Number', 'Founders',\n",
    "             'Transaction Name', 'Contact Job Departments',\n",
    "             'Number of Contacts', 'Number of Private Contacts',\n",
    "             'api_raw', 'gender', 'prob',\n",
    "             'IPO Status', 'Operating Status', 'Diversity Spotlight (US Only)',\n",
    "              'Exit Date', 'Closed Date', 'Company Type', 'Hub Tags',\n",
    "              'Actively Hiring', 'Investor Type', 'Investment Stage',\n",
    "              'Number of Portfolio Organizations','Number of Investments',\n",
    "              'Number of Lead Investments', 'Number of Diversity Investments',\n",
    "              'Number of Exits', 'Number of Exits (IPO)', 'Accelerator Program Type',\n",
    "              'Accelerator Application Deadline', 'Accelerator Duration (in weeks)',\n",
    "              'School Type', 'School Program', 'Number of Enrollments',\n",
    "              'School Method', 'Number of Founders (Alumni)', 'Number of Alumni',\n",
    "              'Acquired by', 'Announced Date', 'Price', \n",
    "              'Acquisition Type', 'Acquisition Terms', 'Acquisition Status',\n",
    "              'IPO Date', 'Delisted Date', 'Money Raised at IPO',\n",
    "              'Valuation at IPO', 'Stock Symbol', 'Stock Exchange', 'Number of Events',\n",
    "              'Last Leadership Hiring Date', 'Last Layoff Mention Date',\n",
    "              'IT Spend', 'Date of Most Recent Valuation', 'Number of Private Notes', \n",
    "              'Most Popular Trademark Class', 'Most Popular Patent Class',\n",
    "              'Tags', 'Unnamed: 107', 'Funding Status',\n",
    "#               'Industries', \n",
    "              'Funding Status', 'Last Equity Funding Type',\n",
    "              #correlated to funding\n",
    "              'CB Rank (Organization)', 'CB Rank (School)', 'CB Rank (Company)',\n",
    "              'Number of Funding Rounds', \n",
    "              'Trend Score (7 Days)', 'Trend Score (30 Days)', 'Trend Score (90 Days)',\n",
    "              'Last Funding Amount', 'Last Equity Funding Amount', 'Total Equity Funding Amount']\n",
    "\n",
    "df.drop(columns=drop_cols, inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 encoding categorical data\n",
    "### 2.2.1 convert text to equal categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**are they equal or ordinal?**\n",
    "- `Funding Status`: \"Early Stage Venture\", \"Seed\", \"M&A\" (overlaps with `Last Funding Type`)\n",
    "- `Acquisition Status`: \"Was Acquired\", \"Made Acquisitions\", \"Made Acquisitions; Was Acquired\" (for early stage too many NULLs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equal_cat(df, col):\n",
    "    \n",
    "    '''create new columns binary encoding each category'''\n",
    "    \n",
    "    # deal with NULL values\n",
    "    new_col = col.lower().replace(' ', '_')\n",
    "    df[new_col] = df[col].str.replace('—',f'{new_col}_null')\n",
    "    \n",
    "    # initiate binary encoder\n",
    "    ohe = OneHotEncoder()\n",
    "    \n",
    "    # join original df with the created df with many new binary columns\n",
    "    df_ohe = pd.DataFrame(ohe.fit_transform(asarray(df[new_col]).reshape(-1,1)).toarray(), \n",
    "                          columns=ohe.categories_, index=df.index)\n",
    "    df_ohe.columns = df_ohe.columns.get_level_values(0)\n",
    "    \n",
    "    # deal with exceptions\n",
    "    try:\n",
    "        df = df.join(df_ohe)\n",
    "    except ValueError:\n",
    "        # country==state\n",
    "        if col == 'hq_state':\n",
    "            df = df.join(df_ohe.drop(columns=state_country))\n",
    "        # state==city\n",
    "        if col == 'hq_city':\n",
    "            df = df.join(df_ohe.drop(columns=state_city))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Location data\n",
    "\n",
    "- `Headquarters Location` _(city, state, country) where many city=state (e.g. New York)_ **(break it down, only take state and country, and use OneHotEncoder)**\n",
    "- `Headquarters Regions` (to avoid overlap with prev, only take last region and use OneHotEncoder)\n",
    "\n",
    "#### preprocess multi-label location columns for one-hot-encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def location_cat(df):\n",
    "    \n",
    "    #1 location data extraction\n",
    "    ##1.1 regions\n",
    "    df['hq_region'] = df['Headquarters Regions'].str.lower().str.strip('').str.split('; ').str[-1]\n",
    "\n",
    "    ##1.2 location\n",
    "    df['hq_country'] = df['Headquarters Location'].str.lower().str.strip('').str.split('; ').str[-1]\n",
    "    df['hq_state'] = df['Headquarters Location'].str.lower().str.strip('').str.split('; ').str[-2]\n",
    "    # df['hq_city'] = df['Headquarters Location'].str.lower().str.strip('').str.split('; ').str[-3]\n",
    "\n",
    "    #2 specific location data level grouping\n",
    "    hq_location = []\n",
    "    for i in range(df.shape[0]):\n",
    "        \n",
    "        ##2.1 if country==us, keep region level\n",
    "        # western us (424), northeastern us (196), west coast (147), \n",
    "        # southern us (121), new england (62), midwestern us (50)\n",
    "        if df['hq_country'][i]=='united states':\n",
    "            us_region = df['hq_region'][i]\n",
    "            \n",
    "            #2.1.1 combine new england into northeastern (258)\n",
    "            if us_region=='new england':\n",
    "                hq_location.append('northeastern us')\n",
    "                \n",
    "            #2.1.2 combine west coast into western (571)\n",
    "            elif us_region=='west coast':\n",
    "                hq_location.append('western us')\n",
    "                \n",
    "            #2.1.3 the rest\n",
    "            else:\n",
    "                hq_location.append(us_region)\n",
    "    \n",
    "    \n",
    "        ##2.2 if country==china, keep state level\n",
    "        if df['hq_country'][i]=='china':\n",
    "            \n",
    "            ###2.2.1 majority states\n",
    "            # beijing (393), shanghai (198), guangdong (179)\n",
    "            if df['hq_state'][i] in ['beijing', 'shanghai', 'guangdong']:\n",
    "                china_state = df['hq_state'][i]\n",
    "                hq_location.append(china_state)\n",
    "            \n",
    "            ###2.2.2 minority states (230)\n",
    "            else:\n",
    "                hq_location.append('chinese_state')\n",
    "\n",
    "                \n",
    "        ##2.3 for scandanavia, also keep region level\n",
    "        if df['hq_region'][i]=='scandinavia':\n",
    "            hq_location.append('scandinavia')\n",
    "\n",
    "        ##2.4 for europe, keep country level\n",
    "        ### 2.4.1 majority countries\n",
    "        # uk (290), france (183), germany (105)\n",
    "        if df['hq_country'][i] in ['united kingdom', 'france', 'germany']:\n",
    "            eup_country = df['hq_country'][i]\n",
    "            hq_location.append(eup_country)\n",
    "        \n",
    "        ### 2.4.2 within eu\n",
    "        elif df['hq_region'][i]=='european union (eu)':\n",
    "            hq_location.append('europe_country')\n",
    "        \n",
    "        ### 2.4.3 outside eu but within europe\n",
    "        elif df['hq_country'][i] in ['switzerland', 'russian federation', \n",
    "                                     'belarus', 'liechtenstein', 'turkey']:\n",
    "            hq_location.append('europe_country')\n",
    "\n",
    "\n",
    "    df.drop(columns=['Headquarters Regions', 'Headquarters Location', \n",
    "                     'hq_region', 'hq_country', 'hq_state'], inplace=True)\n",
    "    df['hq_location'] =  hq_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_cat(df)\n",
    "# this updated location col has no null info\n",
    "# df['hq_location'].value_counts()#.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "beijing            250\n",
       "chinese_state      162\n",
       "guangdong          132\n",
       "shanghai           124\n",
       "europe_country      38\n",
       "united kingdom      26\n",
       "western us          21\n",
       "france              14\n",
       "northeastern us     11\n",
       "scandinavia         10\n",
       "southern us          9\n",
       "midwestern us        4\n",
       "germany              2\n",
       "Name: hq_location, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['%female'].isnull()]['hq_location'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "western us         550\n",
       "europe_country     286\n",
       "united kingdom     264\n",
       "northeastern us    247\n",
       "france             169\n",
       "beijing            143\n",
       "southern us        112\n",
       "germany            103\n",
       "scandinavia         88\n",
       "shanghai            74\n",
       "chinese_state       68\n",
       "guangdong           47\n",
       "midwestern us       46\n",
       "Name: hq_location, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['hq_location'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 convert text to ORDINAL categories\n",
    "\n",
    "- 'Last Funding Type'\n",
    "- 'Estimated Revenue Range'\n",
    "- 'Number of Employees'\n",
    "- 'Last Equity Funding Type'\n",
    "- 'Most Recent Valuation Range'\n",
    "\n",
    "*Note: `.astype('category').cat.codes` is not a good method because it assigns the number in random order*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ordinal_cat(df, col):\n",
    "    \n",
    "    '''create one new column with ordinal categories'''\n",
    "    \n",
    "    # get text for new column name\n",
    "    new_col = col.lower().replace(' ', '_')\n",
    "    \n",
    "    \n",
    "    # specify ordinal order\n",
    "    if (col=='Last Funding Type') or (col=='Last Equity Funding Type'):\n",
    "        labels = ['Seed', 'Series A']\n",
    "    \n",
    "    if (col=='Estimated Revenue Range') or (col=='Most Recent Valuation Range'):\n",
    "        labels = ['—', 'Less than $1M', '$1M to $10M', '$10M to $50M', \n",
    "                  '$50M to $100M', '$100M to $500M', '$500M to $1B', \n",
    "                  '$1B to $10B', '$10B+']\n",
    "    \n",
    "    if col == 'Number of Employees':\n",
    "        # some '1-10' were read incorrectly and automatically converted to date formats\n",
    "        df['Number of Employees'] = df['Number of Employees'].str.replace('10-Jan', '1-10')\n",
    "        labels = ['—', '1-10', '11-50', '51-100', '101-250', '251-500', \n",
    "                  '501-1000', '1001-5000', '5001-10000', '10001+']\n",
    "    \n",
    "    \n",
    "    # convert text to ordinal categories\n",
    "    cat = list(np.array(labels).reshape(1,len(labels)))\n",
    "    oe = OrdinalEncoder(categories=cat)\n",
    "    df[new_col] = oe.fit_transform(asarray(df[col]).reshape(-1, 1))\n",
    "    df[new_col] = df[new_col].astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 convert text list to MULTI-LABEL categories\n",
    "#1\n",
    "- `Headquarters Location` (after processing moved to OneHotEncoder)\n",
    "- `Headquarters Regions` (after processing moved to OneHotEncoder)\n",
    "\n",
    "#2\n",
    "- `Industry Groups` \n",
    "- `Industries` (not ignored because does not overlaps with `Industry Groups`)\n",
    "\n",
    "*but also they cannot both be processed because collinearity*\n",
    "\n",
    "note: [Industry Group v Industries Table (Crunchbase)](https://support.crunchbase.com/hc/en-us/articles/360043146954-What-Industries-are-included-in-Crunchbase-)\n",
    "\n",
    "#3\n",
    "- `Top 5 Investors` (after processing moved to OneHotEncoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### industry group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all industry_groups\n",
    "all_industry_groups = []\n",
    "df_industry_groups = df['Industry Groups'].str.lower().str.strip('').str.split('; ')\n",
    "for i in range(3000):\n",
    "    all_industry_groups.extend(df_industry_groups[i])\n",
    "    \n",
    "# len(all_industry_groups) #10477\n",
    "# len(set(all_industry_groups)) #48\n",
    "\n",
    "top_industry_groups = []\n",
    "for key, val in Counter(all_industry_groups).items():\n",
    "    if val >= 480:\n",
    "        top_industry_groups.append(key)\n",
    "        \n",
    "# len(top_industry_groups) #6\n",
    "top_industry_groups #['health care', 'science and engineering', 'internet services', \n",
    "                      # 'software', 'data and analytics', 'information technology']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_industry_group_bool(df):\n",
    "    # extract data\n",
    "    df['industry_groups'] = df['Industry Groups'].str.lower().str.strip('').str.split('; ')\n",
    "    \n",
    "    # get data\n",
    "    group_lst = []\n",
    "    for row in range(3000):\n",
    "        val = 0\n",
    "        for i in df['industry_groups'][row]:\n",
    "            if i in top_industry_groups:\n",
    "                val = 1\n",
    "        group_lst.append(val)\n",
    "\n",
    "    # create new col\n",
    "#     df.drop(columns=['Industry Groups', 'industry_groups'], inplace=True)\n",
    "    df['top_group_bool'] = group_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_industry_group_bool(df)\n",
    "# df['top_group_bool'].value_counts() #1: 2187, 0: 813"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### industries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all industries\n",
    "all_industries = []\n",
    "df_industries = df['Industries'].str.lower().str.strip('').str.split('; ')\n",
    "for i in range(3000):\n",
    "    all_industries.extend(df_industries[i])\n",
    "    \n",
    "# len(all_industries) #11036\n",
    "# len(set(all_industries)) #603\n",
    "\n",
    "top_industries = []\n",
    "for key, val in Counter(all_industries).items():\n",
    "    if val >=50:\n",
    "        top_industries.append(key)\n",
    "        \n",
    "len(top_industries) #51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "half_industries = []\n",
    "for key, val in Counter(all_industries).items():\n",
    "    if val >= 5:\n",
    "        half_industries.append(key)\n",
    "half_industries.remove('—')\n",
    "len(half_industries) #50: count=51--> 40: count=61, 30: count=77; 20: count=126; 10: count=206; 5: count=324"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_industries_count(df):\n",
    "    # extract data\n",
    "    df['industries'] = df['Industries'].str.lower().str.strip('').str.split('; ')\n",
    "    \n",
    "    # get data\n",
    "    count_lst = []\n",
    "    for row in range(3000):\n",
    "        tot = len(df['industries'][row])\n",
    "        val = 0\n",
    "        for i in df['industries'][row]:\n",
    "            if i in top_industries:\n",
    "                val += 1\n",
    "        count_lst.append(val/tot)\n",
    "\n",
    "    # create new col\n",
    "#     df.drop(columns=['Industries', 'industries'], inplace=True)\n",
    "    df['top_industry_count'] = count_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_industries_count(df)\n",
    "# df['top_industry_count'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['top_industry_count'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### investors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all investors\n",
    "all_investors = []\n",
    "df_investors = df['Top 5 Investors'].str.lower().str.strip('').str.split('; ')\n",
    "for i in range(3000):\n",
    "    all_investors.extend(df_investors[i])\n",
    "    \n",
    "# len(all_investors) #8905\n",
    "# len(set(all_investors)) #4773\n",
    "\n",
    "# sorted(Counter(all_investors).items(), key=lambda pair: pair[1], reverse=True)\n",
    "# investors in at least 5 companies out of 3000\n",
    "top_investors = []\n",
    "top_top_investors = []\n",
    "for key, val in Counter(all_investors).items():\n",
    "    if val >= 3:\n",
    "        top_investors.append(key)\n",
    "    if val >= 5:\n",
    "        top_top_investors.append(key)\n",
    "\n",
    "# remove nan from list\n",
    "top_investors.remove('—')\n",
    "top_top_investors.remove('—')\n",
    "\n",
    "# one hot encode all these\n",
    "len(top_investors) #val-2: 1318; val=3: 681; val=5: 286; val=10: 78"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_investors_col(df):\n",
    "    # extract data\n",
    "    df['investors'] = df['Top 5 Investors'].str.lower().str.strip('').str.split('; ')\n",
    "    \n",
    "    # get bool data\n",
    "    bool_lst = []\n",
    "    for row in range(3000):\n",
    "        val = 0\n",
    "        for i in df['investors'][row]:\n",
    "            if i in top_top_investors:\n",
    "                val = 1\n",
    "        bool_lst.append(val)\n",
    "        \n",
    "    # get count data\n",
    "    count_lst = []\n",
    "    for row in range(3000):\n",
    "        tot = len(df['investors'][row])\n",
    "        val = 0\n",
    "        for i in df['investors'][row]:\n",
    "            if i in top_investors:\n",
    "                val += 1\n",
    "        count_lst.append(val/tot)\n",
    "\n",
    "    # create new col\n",
    "#     df.drop(columns=['Top 5 Investors', 'investors'], inplace=True)\n",
    "    df['top_investors_bool'] = bool_lst\n",
    "    df['top_investors_count'] = count_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "investors_list = []\n",
    "for key, val in Counter(all_investors).items():\n",
    "    if val >= 15:\n",
    "        investors_list.append(key)\n",
    "\n",
    "len(investors_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_investors_col(df)\n",
    "# df['top_investors_bool'].value_counts() #0: 1517, 1: 1483\n",
    "# df['top_investors_count'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {k: v for k, v in sorted(Counter(all_industry_groups).items(), key=lambda item: item[1], reverse=True)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multilabel_cat(df, col):\n",
    "    '''create multiple one-hot encoded columns for each tag/label in a row'''\n",
    "    \n",
    "    # dealing with null valuess (so that null_cols for each newly created col is a different name)\n",
    "    new_col = col.lower().replace(' ', '_')\n",
    "    df[new_col] = df[col].str.replace('—', f'{new_col}_null')\n",
    "    \n",
    "    # get list of labels from text in each row\n",
    "    df[f'{new_col}_lst'] = df[new_col].str.lower().str.strip('').str.split('; ')\n",
    "    \n",
    "    # initiate multi-label binary encoder\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    \n",
    "    # join original df with the created df with many new binary columns\n",
    "    df_mlb = pd.DataFrame(mlb.fit_transform(df[f'{new_col}_lst']),\n",
    "                          columns=mlb.classes_, index=df.index)\n",
    "    \n",
    "    # only take top info to add back to table because investors info is sparse\n",
    "    if col=='Top 5 Investors':\n",
    "#         df_mlb = df_mlb[top_top_investors]\n",
    "        df_mlb = df_mlb[top_investors]\n",
    "        \n",
    "    if col=='Industries':\n",
    "        df_mlb = df_mlb[half_industries]\n",
    "    \n",
    "    df = df.join(df_mlb)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 convert text to separate dates\n",
    "(1) have full date (format, e.g. \"Dec 31; 1999\"), \n",
    "(2) some have full date but most only have year\n",
    "\n",
    "- `Last Funding Date`: (1)\n",
    "- `Founded Date`: (2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_date(df, col):\n",
    "    \n",
    "    '''create new columns separating date into day, month, year'''\n",
    "    \n",
    "    # (1) have full date info (format, e.g. \"Dec 31; 1999\")\n",
    "    if all(df[col].str.len()>10):\n",
    "    \n",
    "        # get text for new column name\n",
    "        new_col1 = col.lower().replace(' ', '_').replace('date', 'day')\n",
    "        new_col2 = col.lower().replace(' ', '_').replace('date', 'month')\n",
    "        new_col3 = col.lower().replace(' ', '_').replace('date', 'year')\n",
    "\n",
    "        # convert day and year\n",
    "        df[new_col3] = df[col].str[-4:]\n",
    "        df[new_col1] = df[col].str[3:5]\n",
    "\n",
    "        # convert month\n",
    "        # df[new_col2] = df[col].str[:3] #text\n",
    "        df[new_col2] = pd.to_datetime(df[col].str[:3], format='%b').dt.month\n",
    "    \n",
    "    \n",
    "    # (2) some rows have full date but most only have year info\n",
    "    else:\n",
    "        \n",
    "        # get text for new column name\n",
    "        new_col = col.lower().replace(' ', '_').replace('date', 'year')\n",
    "\n",
    "        # convert day and year\n",
    "        df[new_col] = df[col].str[-4:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 convert text to number\n",
    "\n",
    "1. integer\n",
    "2. float (percentage)\n",
    "3. currency (multiply and union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_num(df, col, type='int'):\n",
    "    \n",
    "    '''update original column converting text to appropriate numerical format'''\n",
    "    \n",
    "    # get new column name\n",
    "    new_col = col.lower().replace(' ', '_')\n",
    "    \n",
    "    # common cleaning: deal with NULL values\n",
    "    df[new_col] = df[col].str.replace('—','0')\n",
    "    \n",
    "    # (1) integer\n",
    "    if type=='int':\n",
    "        \n",
    "        # convert text to int\n",
    "        df[new_col] = df[new_col].str.replace(';','').astype('int')\n",
    "        \n",
    "    # (2) float (percentage)\n",
    "    if type=='float':\n",
    "        \n",
    "        # additional step to strip sign\n",
    "        df[new_col] = df[new_col].str.replace('%','')\n",
    "        \n",
    "        # convert text to float\n",
    "        df[new_col].str.replace(';','').astype('float')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_curr(df, col):\n",
    "    '''create new column converting all amount to USD'''\n",
    "    \n",
    "    # get new column name\n",
    "    new_col = col.lower().replace(' ', '_')\n",
    "    \n",
    "    # clean text\n",
    "    df[new_col] = df[col].str.replace(';','')\n",
    "    \n",
    "    # add new col \"conversion rate\" of usd:currency = 1:x\n",
    "    df['cvr'] = 0\n",
    "    \n",
    "    # strip currency signs and update conversion rate\n",
    "    # us dollar\n",
    "    df[new_col] = df[new_col].str.replace('$','')\n",
    "    df.loc[df[col].str[0]=='$', 'cvr'] = 1\n",
    "    \n",
    "    # euro\n",
    "    df[new_col] = df[new_col].str.replace('€','')\n",
    "    df.loc[df[col].str[0]=='€', 'cvr'] = 1.1\n",
    "    \n",
    "    # uk pound\n",
    "    df[new_col] = df[new_col].str.replace('£','')\n",
    "    df.loc[df[col].str[0]=='£', 'cvr'] = 1.34\n",
    "    \n",
    "    # japanese yen\n",
    "    df[new_col] = df[new_col].str.replace('¥','')\n",
    "    df.loc[df[col].str[0]=='¥', 'cvr'] = 0.0087\n",
    "    \n",
    "    # chinese yuan ('CN¥')\n",
    "    df[new_col] = df[new_col].str.replace('CN','')\n",
    "    df.loc[df[col].str[0:2]=='CN', 'cvr'] = 0.16\n",
    "    \n",
    "    # canadian dollar ('CA$')\n",
    "    df[new_col] = df[new_col].str.replace('CA','')\n",
    "    df.loc[df[col].str[0:2]=='CA', 'cvr'] = 0.79\n",
    "    \n",
    "    # swiss franc\n",
    "    df[new_col] = df[new_col].str.replace('CHF','')\n",
    "    df.loc[df[col].str[0:3]=='CHF', 'cvr'] = 1.09\n",
    "    \n",
    "    # swedish krona\n",
    "    df[new_col] = df[new_col].str.replace('SEK','')\n",
    "    df.loc[df[col].str[0:3]=='SEK', 'cvr'] = 0.1\n",
    "    \n",
    "    # russian ruble\n",
    "    df[new_col] = df[new_col].str.replace('RUB','')\n",
    "    df.loc[df[col].str[0:3]=='RUB', 'cvr'] = 0.01\n",
    "        \n",
    "    # norwegian krone\n",
    "    df[new_col] = df[new_col].str.replace('NOK','')\n",
    "    df.loc[df[col].str[0:3]=='NOK', 'cvr'] = 0.11\n",
    "    \n",
    "    # new zealand dollar ('NZ$')\n",
    "    df[new_col] = df[new_col].str.replace('NZ','')\n",
    "    df.loc[df[col].str[0:2]=='NZ', 'cvr'] = 0.69\n",
    "    \n",
    "    # poland ztoty\n",
    "    df[new_col] = df[new_col].str.replace('PLN','')\n",
    "    df.loc[df[col].str[0:3]=='PLN', 'cvr'] = 0.24\n",
    "        \n",
    "    # icelandic krona\n",
    "    df[new_col] = df[new_col].str.replace('ISK','')\n",
    "    df.loc[df[col].str[0:3]=='ISK', 'cvr'] = 0.008\n",
    "    \n",
    "    # hungarian forint\n",
    "    df[new_col] = df[new_col].str.replace('HUF','')\n",
    "    df.loc[df[col].str[0:3]=='HUF', 'cvr'] = 0.003\n",
    "    \n",
    "    # null value\n",
    "    df[new_col] = df[new_col].str.replace('—','0')\n",
    "    \n",
    "    \n",
    "    '''cannot strip currency and convert to int the multipl only for parts of the data \n",
    "       so the best implementation is to split it into two steps'''\n",
    "    \n",
    "    # multiply number by conversion rate to get amount all in usd\n",
    "    df[new_col] = df[new_col].astype('int')\n",
    "    df[f'{new_col}_usd'] = df[new_col]*df['cvr']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 convert text to NLP (bag of words?)\n",
    "- `Description`\n",
    "- `Full Description`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run all conversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# headquarters info all moved to equal_cats\n",
    "equal_cats = ['hq_location'] #'hq_region', 'hq_country', 'hq_state']#, 'hq_city']\n",
    "for cat1 in equal_cats:\n",
    "    df = equal_cat(df, cat1)\n",
    "    print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ord_cats = ['Last Funding Type', 'Estimated Revenue Range', 'Number of Employees', \n",
    "            'Most Recent Valuation Range']\n",
    "for cat2 in ord_cats:\n",
    "    ordinal_cat(df, cat2)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_cols = ['Last Funding Date', 'Founded Date']\n",
    "for date_col in date_cols:\n",
    "    text_date(df, date_col)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_cols = ['Number of Articles', 'Number of Lead Investors', \n",
    "            'Number of Investors', 'Number of Acquisitions', 'Monthly Visits', \n",
    "            'Visit Duration', 'Global Traffic Rank', 'Monthly Rank Change (#)', \n",
    "            'Active Tech Count', 'Number of Apps', 'Downloads Last 30 Days',\n",
    "            'Total Products Active', 'Patents Granted', 'Trademarks Registered']\n",
    "for num1 in int_cols:\n",
    "        text_num(df, num1, type='int')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "float_cols = ['Monthly Visits Growth', 'Visit Duration Growth', 'Page Views / Visit', \n",
    "              'Page Views / Visit Growth', 'Bounce Rate', 'Bounce Rate Growth', \n",
    "              'Monthly Rank Growth', 'Average Visits (6 months)']\n",
    "for num2 in float_cols:\n",
    "    text_num(df, num2, type='float')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_cols = ['Total Funding Amount']\n",
    "for num3 in curr_cols:\n",
    "    text_curr(df, num3)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_cats = ['Top 5 Investors', 'Industries']#, 'Industry Groups']\n",
    "for cat3 in multi_cats:\n",
    "    df = multilabel_cat(df, cat3)\n",
    "    print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# redundant cols generated from feature engineering\n",
    "multi_cats_lst = []\n",
    "for col in multi_cats:\n",
    "    new_col = col.lower().replace(' ', '_')\n",
    "    multi_cats_lst.append(f'{new_col}_lst')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### remove additional cols\n",
    "- old cols that is no longer needed after new processing\n",
    "- midway processing cols used to produce new cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_cols = equal_cats + ord_cats + multi_cats + multi_cats_lst + date_cols + int_cols + float_cols + curr_cols + ['cvr', 'total_funding_amount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=old_cols, inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Post-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "since no longer a classification task, `%female` can be kept as variable!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# also drop the col that would give away\n",
    "df.drop(columns=['#female'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">\n",
    "encode no info as 0.5 in company (so it is a neutral situation? better than encoding 0?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['%female'].fillna(0.5, inplace=True)\n",
    "df.dropna(subset=['%female'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode as bool\n",
    "# df['female_led'] = (df['%female']>0.5).astype(int)\n",
    "# df.drop(columns=['%female'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dealing with missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['total_funding_amount_usd'].isnull().sum()#value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['total_funding_amount_usd']==0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set name as index \n",
    "so that the rest of the columns are all numerical data that could fit in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index('Organization Name', inplace=True)\n",
    "num_cols = df.describe().columns #this takes awhile to load\n",
    "new_df = df[num_cols]\n",
    "new_df.shape #2197*0.5=1098 so cols should be less than that (.5 is test data size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set(df.columns).difference(set(new_df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### export data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df.to_csv('../data/feature_engineering/combined_feng_v11.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
